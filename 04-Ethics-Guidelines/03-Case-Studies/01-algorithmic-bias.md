# AI 알고리즘 차별 사례 분석

## 1. 채용 AI 차별 사례

### Amazon 채용 AI (2015-2018)
- **사례 개요**
  - Amazon의 이력서 스크리닝 AI 시스템
  - 여성 지원자 차별 문제 발생
  - 결국 프로젝트 중단

- **문제점**
  - 과거 채용 데이터의 성별 편향성
  - 특정 단어(women's, female 등) 기반 차별
  - 기술직 지원자의 성별 불균형 반영

- **시사점**
  - 학습 데이터의 역사적 편향성 주의
  - 명시적/암묵적 차별 요소 제거
  - 지속적인 모니터링 필요

## 2. 금융 AI 차별 사례

### Apple Card 신용한도 차별 (2019)
- **사례 개요**
  - 동일 조건에서 성별에 따른 신용한도 차이
  - 부부간 큰 신용한도 차이 발생
  - 소셜미디어 통해 이슈화

- **문제점**
  - 신용평가 알고리즘의 성별 편향
  - 평가 기준의 불투명성
  - 설명 가능성 부족

- **시사점**
  - 알고리즘 공정성 검증 필요
  - 의사결정 과정 투명성 확보
  - 이의제기 절차 마련

## 3. 얼굴인식 AI 차별 사례

### 구글 포토 인종 차별 (2015)
- **사례 개요**
  - 흑인을 고릴라로 태깅하는 사고
  - 인종적 편향성 문제 제기
  - 대대적인 사과와 수정

- **문제점**
  - 학습 데이터의 인종 다양성 부족
  - 품질 관리 체계 미흡
  - 사회적 맥락 고려 부족

- **시사점**
  - 다양한 인종 데이터 확보
  - 문화적 감수성 고려
  - 출시 전 철저한 검증

## 4. 공공부문 AI 차별 사례

### COMPAS 재범예측 알고리즘
- **사례 개요**
  - 미국 법원의 재범 위험도 평가 시스템
  - 흑인에 대한 높은 재범 위험도 예측
  - ProPublica의 조사로 편향성 밝혀짐

- **문제점**
  - 인종별 재범률 예측 편향
  - 알고리즘 블랙박스 문제
  - 사회적 편견 강화

- **시사점**
  - 공공 AI의 특별한 책임
  - 알고리즘 감사 필요성
  - 인권영향평가 중요성

## 5. 차별 예방을 위한 교훈

### 개발 단계
1. **데이터 품질**
   - 대표성 있는 데이터 수집
   - 편향성 검사 및 제거
   - 지속적인 데이터 감사

2. **알고리즘 설계**
   - 공정성 메트릭스 도입
   - 편향성 완화 기술 적용
   - 설명가능성 확보

3. **테스트**
   - 다양한 그룹 대상 테스트
   - 편향성 정기 검사
   - 사회적 영향 평가

### 운영 단계
1. **모니터링**
   - 성능 지표 추적
   - 피드백 수집
   - 정기적 감사

2. **대응 체계**
   - 이의제기 절차
   - 신속한 조치
   - 투명한 소통

3. **개선 활동**
   - 지속적 학습
   - 알고리즘 업데이트
   - 프로세스 개선

## 6. 권장 대응 방안

### 기술적 방안
1. **데이터 관리**
   - 데이터 품질 기준 수립
   - 편향성 검사 도구 활용
   - 정기적 데이터 감사

2. **알고리즘 개선**
   - 공정성 메트릭스 도입
   - 설명가능성 향상
   - 테스트 강화

### 정책적 방안
1. **거버넌스**
   - 윤리 위원회 설립
   - 감독 체계 구축
   - 정기 평가 제도

2. **프로세스**
   - 개발 가이드라인
   - 검증 절차
   - 사고 대응 체계

## 7. 향후 과제

### 연구 개발
- 공정성 메트릭스 개선
- 설명가능한 AI 기술
- 편향성 감지 도구

### 제도 개선
- 법적 기준 마련
- 감사 체계 구축
- 인증 제도 도입

## 참고 문헌
1. ProPublica COMPAS 분석 보고서
2. AI Now Institute 연구 보고서
3. 각 사례 관련 언론 보도

## 업데이트 이력
- 2024-12-17: 초기 문서 작성
